\section{Background and Motivation}
\label{sec:motivation}

In this section, we introduce the background of container technology and motivate our system.
Container technology has gain tremendous popularity since it enables a fast and less-dependent
way to distribute, deploy or test applications and services.
Compared with applications within processes, container provides better isolation as the dependencies of containerized applications are
wrapped together with the application, thus can be easily distributed and deployed.
Compared with applications inside VMs, container are much more lightweight and can be redeployed and tested in a faster and
less expensive manner. Therefore, containers are able to be deployed in more diverse location than VM
(e.g., on bare-metal or inside VM).
Next we introduce the container applications that motivates our system.

\subsection{Containerized applications}

\begin{figure*}[h]  
	\centering   
	\includegraphics[width=6.7in]{figures/deployment-cases}   
	\caption{\label{fig:deploy-cases} Representative running environments of containers.}   
\end{figure*}

%\begin{table} [t!]
%\centering
%\small
%\begin{tabular}{ c || c | c | c | c }
%  \hline
%  Constraint & Case (a) & Case (b) & Case (c) & Case (d) \\ \hline \hline
%  N/A & SharedMem & RDMA & SharedMem & RDMA \\ \hline
%  w/o trust & TCP/IP & TCP/IP & TCP/IP & TCP/IP \\ \hline
%  w/o RDMA NIC & SharedMem & TCP/IP & SharedMem & TCP/IP \\ \hline
%\end{tabular}
%\caption{\label{tab:best-network} The suggested network solution under %different running environments in Figure~\ref{fig:deploy-cases} and constraints.}
%\normalsize
%\end{table}

Nowadays, a containerized application is usually composed by multiple containers. For example, each master and slave node in Hadoop is an individual container; A web service can include layers, such as load balancer, web server,
in-memory cache and backend database, and each layer can be a distributed 
system with multiple containerized nodes. 
These containers are usually 
deployed into a multi-host server cluster, and the deployment is usually 
controlled by a cluster orchestrator, e.g. Mesos~\cite{?}. 
Such architecture makes it easier
to upgrade the nodes or mitigate failures, since a stopped container can be quickly replaced by a new one on the same or another host.

Working as a single application, containers need to exchange data, and the network performance has a huge impact on the overall application performance.
Depending on whether 
containers run on bare-metal hosts or VM hosts, Figure~\ref{fig:deploy-cases}
illustrates four representative cases for a container networking solutions
to handle. 
%As shown later in this section, one networking solution has 
%different efficiency and feasibility in different cases. 
In this paper, 
we focus on the cases (a) and (b) in Figure~\ref{fig:deploy-cases} and discuss how to support the other two cases in the future.

%We set up two iPerf containers running in server and client roles respectively and benchmark the network bandwidth between them when they use three different networking modes:
%\vyas{2.1 has to be about background and  the status quo. not about your experiment. i would make 2.2 your experiment setup details etc}
%\harry{Sorry, I copied and pasted some text from intro. Have not cleaned the Section yet.}

%\begin{itemize}
%	\item {\em Host}, in which one container binds an interface
%and a port on the host and use the host's IP to communicate, like an ordinary
%process.
%	\item {\em Bridge}, in which all containers on the same host are connected to a common network bridge in OS kernel. It is only used for
%	intra-host communications.  
%	\item {\em Overlay}, in which each host runs a software router which
%	connects all containers on the host via bridge network. An overlay 
%	network crossing multiple hosts is constructed by the software routers
%	to achieve a uniform IP assignment and traffic routing on the overlay.
%\end{itemize}


\subsection{Existing container overlay networks}

\vyas{seems like a logical disconnect to me .. why is there a section on overlay networks?}

\vyas{i would keep it simple as follows: 2.1 -- background 2.2 -- measurements 2.3 -- insights. dont get too fancy 
 in subsection titles/structure}

\begin{figure}[h]  
	\centering   
	\includegraphics[width=0.8\linewidth]{figures/overlay.pdf}   
	\caption{\label{fig:overlay} The design of existing container overlay networks (Weave).}   
\end{figure}   

As mentioned in \S\ref{sec:introduction}, {\em Overlay} networking mode 
is preferred to keep the portability and independence of containers.
Despite there are multiple available container overlay networking solutions
in the state of art, e.g. Docker Overlay Network, Weave, Calico, etc., 
their basic idea for offering the connectivity to the containers is similar.
Figure~\ref{fig:overlay} shows the architecture of an overlay container network.
Each hosts runs a software router that connects to one NIC of the host. 
Leveraging the connectivity provided by the inter-host network, the routers
on different hosts can exchange information and speak standard routing protocols.
Inside each host, a virtual interface (veth0) is created in each container and is connected to a virtual interface of the software router in the same host via a software network bridge. The virtual interface of a container is assigned
an overlay IP (e.g. 1.1.1.1 to Container-1) and the host of this overlay IP
is propagated among the software routers in the cluster. Therefore, containers
can send and receive IP packets merely with their overlay IPs, agnostic to
the real location of the other side. For instance, when a IP packet from
Container-1 to Container-3 is send to Router-1, the latter knows 3.3.3.3
(Container-3's overlay IP) is on Host-2, so that Router-1 will encapsulate 
an IP header with destination IP 192.168.1.2 (Host-2's IP) on top of the packet
and send it to Host-2. Host-2 decapsulates the packet after receiving it
and deliver it to Container-3.  

There are two bottlenecks which significantly limit the performance
of current container overlay networks. The first one is the network bridge
connecting containers and software routers. It is a software implementation
of a subset of the ANSI/IEEE 802.1d standard~\cite{?} which forwards packets
according to Ethernet addresses. Tremendous CPU resource is spent on matching
addresses and data copying. The second bottleneck is the software router, since
CPUs are used to performance the encapsulations, decapsulations and packet matching/forwarding.


\subsection{Opportunities for better networking}

The key to achieve a high performance and low overhead overlay network
for containers is bypassing the performance bottlenecks, including
bridges, software routers and host OS kernel, on data-plane. Given that
containers are essentially processes, the communication channels provided
by IPC and hardware offloading techniques give us numerous hammers to 
build a better container network. For instance, 
containers within a single host (like Container-1 and 
Container-2 in Figure~\ref{fig:overlay}) can communicate via 
shared-memory and software routers in different hosts can talk via RDMA, 
which helps to bypass performance bottlenecks.

To understand the network performance and overhead when containers
talk via shared-memory and RDMA \footnote{In this paper, we choose RDMA
as a representative hardware offloading technique, and we will consider other
alternatives in the future.}, we conducted network bandwidth and latency benchmarks between two containers. In the shared-memory case, the sender
container opens a pieces of shared memory and write data into it, and 
the receiver container can read the memory once it gets the pointer of 
the shared memory from the sender; In the RDMA case, both the sender
and the receiver use host modes and communicate with the standard RDMA API
over the host NICs. We use Weave to in the evaluation of existing overlay
networks. The results are shown in Figure XXX.

     \begin{figure}[ht]
     \centering 
     \includegraphics[width=0.5\textwidth]{figures/motivation/mot_rdma_shm.pdf}      
     \label{fig:mot_rdma_shm}
     \caption{The throughput, CPU usage, latency and NIC utilization for a pair of 
     containers communicating via overlay network, RDMA or shared memory.} 
     \end{figure}

Figure~\ref{fig:eval_baremetal_thr} shows that in intra-host cases, the bandwidth between two
containers can reach the NIC speed at about 40Gbps then RDMA is used and 
even exceed the limitation of NIC speed to approach to memory bandwidth when
they use shared-memory is used. As shown in Figure The price of the high bandwidth of shared-memory is high CPU utilization due to data-copying when the sender writes data into
the shared-memory, while RDMA has negligible CPU overhead because it directly uses NICs and DMA controllers for data copying and packet processing.
Figure~\ref{fig:eval_exist_latency} show the latency when a sender 
transfers XXX bytes to the receiver. We can see that both shared-memory
and RDMA have XXX orders of magnitudes reduction in latency compared with Weave.
Overall, we can conclude that both shared-memory and RDMA have 
significant improvement in network performance and reduction in CPU overhead,
compared with existing overlay networking solutions.

     \begin{figure}[ht]
     \centering 
     \includegraphics[width=0.5\textwidth]{figures/motivation/mot_rdma_inter.pdf}      
     \label{fig:mot_rdma_inter}
     \caption{The throughput, CPU usage, latency and NIC utilization for a pair of 
     containers communicating via overlay network, RDMA or shared memory in 
     inter-host setting.} 
     \end{figure}
     
 \tianlong{Here we need a paragraph say for inter-host setting, we can use RDMA to achieve high bw, low latency.}

Therefore, we have a huge opportunity to achieve a high performance
and low overhead network if we can enable shared-memory and RDMA 
in the data-plane of container overlay networks.

\subsection{Challenges}

(1) Using standard APIs, transparent to containers;


(2) Smoothly integrating shared-memory and RDMA according to containers' locations.

(3) Optimizations like removing unnecessary data copies.

%%% commented content 
\iffalse

%Throughput:
     \begin{figure}[ht]
     \centering 
     \includegraphics[width=0.32\textwidth]{figures/motivation/eval_baremetal_thr.pdf}      
     \label{fig:eval_baremetal_thr}
     \caption{} 
     \end{figure}

%Latency:    
     \begin{figure}[ht]
     \centering 
     \includegraphics[width=0.32\textwidth]{figures/motivation/eval_baremetal_latency.pdf}      
     \label{fig:eval_baremetal_latency}
     \caption{} 
     \end{figure}

%CPU usage:     
     \begin{figure}[ht]
     \centering 
     \includegraphics[width=0.32\textwidth]{figures/motivation/eval_baremetal_cpu.pdf}      
     \label{fig:eval_baremetal_cpu}
     \caption{} 
     \end{figure}

\begin{figure}[ht]
     \centering 
     \includegraphics[width=0.4\textwidth]{figures/motivation/eval_exist_bw.pdf} 
     \label{fig:eval_exist_bw}
     \caption{The intra-host throughput of existing solutions. Docker-host is in host mode; Docker0 is in bridge mode; Weave is in overlay mode.} 
\end{figure} 

\begin{figure}[ht]
     \centering 
     \includegraphics[width=0.4\textwidth]{figures/motivation/eval_exist_latency.pdf} 
     \label{fig:eval_exist_latency}
     \caption{The intra-host latency of existing solutions. Docker-host is in host mode; Docker0 is in bridge mode; Weave is in overlay mode.} 
\end{figure} 

\begin{figure}[ht]
     \centering 
     \includegraphics[width=0.4\textwidth]{figures/motivation/eval_exist_cpu.pdf} 
     \label{fig:eval_exist_cpu}
     \caption{The cpu usage of existing solutions. Docker-host is in host mode; Docker0 is in bridge mode; Weave is in overlay mode.} 
\end{figure} 

\begin{figure}[ht]
     \centering 
     \includegraphics[width=0.4\textwidth]{figures/motivation/eval_bw_cpu.pdf}      
     \label{fig:eval_bw_cpu}
     \caption{} 
\end{figure}


* Figures:

* Throughput comparison (intra- and inter-host)

* CPU comparison

* Latency comparison

In this section, we present measurement results that demonstrates the network performance and resource overhead of different optional network channels. We focus on network throughput and latency when containers are located in the same or different hosts. 




\subsection{Opportunities}

\subsection{Challenges}

\subsection{The ineffiencies in container networking}

We measure the throughput and latency of TCP/IP, Shared-Mem and RDMA under cases
(a) (b) (c) and (d).

\harry{RDMA and Shared-Mem is missing under (c) and (d)}.

\para{The measurement setup}
(1) two bare metals: 40Gbps NICs.
(2) two VMs on top of the two bare metals.
(3) two VMs from Azure or EC2.

\subsection{Intra-host performance}
\subsubsection{throughput vs. latency vs. cpu usage}

\begin{figure*}[t]
     \centering 
     %\begin{subfigure}[t]{0.25\textwidth}
     \begin{subfigure}[t]
     \centering 
     \includegraphics[width=0.32\textwidth]{figures/motivation/eval_baremetal_thr.pdf}      
     %\label{fig:eval_baremetal_thr}
     %\caption{} 
     \end{subfigure}      
     \begin{subfigure}[t]
     \centering 
     \includegraphics[width=0.32\textwidth]{figures/motivation/eval_baremetal_latency.pdf}      
     %\label{fig:eval_baremetal_latency}
     %\caption{} 
     \end{subfigure}      
     \begin{subfigure}[t]
     \centering 
     \includegraphics[width=0.32\textwidth]{figures/motivation/eval_baremetal_cpu.pdf}      
     %\label{fig:eval_baremetal_cpu}
     %\caption{} 
     \end{subfigure}           
     \label{fig:eval_baremetal_thr_latency_cpu}
     \caption{} 
\end{figure*} 

\begin{itemize}
  \item Communication via IP stack only achieves 27Gb/s throughput, 1 ms latency but uses near to 200\% of cpu.
  \item RDMA only improves the throughput to 40Gb/s for containers on the same machine, the latency is still 1ms, though it has a low cpu usage.
  \item Shared memory can achieve near-to-memory-bandwidth throughput, lowest latency, but still burns some cpu. 
\end{itemize}


\subsubsection{Host-mode vs. bridge mode}

\begin{figure}[!ht]
     \centering 
     \includegraphics[width=3.35in]{figures/motivation/eval_bw_host_bridge.pdf} 
     \label{fig:eval_bw_host_bridge}
     \caption{ Host-mode vs. bridge mode vs. RDMA vs. shared memory.} 
\end{figure} 

\begin{itemize}
  \item Host-mode provides a better performance of 38 Gb/s. 
\end{itemize}

\subsection{Intra-host Network Performance}

\para{Throughput}

Three points: (1) TCP throughput is limited; (2) the bottleneck is CPU rather than memory bus for TCP/IP; (3) the bottleneck is NIC CPU for RDMA. (4) For intra-host cases, shared memory has the best performance.

Figure 1: throughput of single src-dst pair. Bar figure: x-axis: TCP/IP, RDMA and shared memory; y-axis: throughput;

Figure 2(a): throughput of multiple src-dst pair. Line figure: x-axis: number of pairs,; y-axis: throughput; Four lines: TCP/IP, RDMA, shared memory and memory bus.

Figure 2(b): CPU utilization. Line figure: x-axis: number of pairs,; y-axis: CPU utilization; Three lines: TCP/IP, RDMA, shared memory.

Figure 2(c): NIC CPU utilization. Line figure: x-axis: number of pairs,; y-axis: NIC CPU utilization; Three lines: TCP/IP, RDMA, shared memory.

\begin{figure}[!ht]
     \centering 
     \includegraphics[width=3.35in]{figures/motivation/eval_baremetal_thr.pdf} 
     \caption{\label{fig:eval_baremetal_thr} The throughput of a pair of containers on the same bare metal communicating via IP stack, RDMA and shared memory. Communication via shared memory is close to the memory bandwidth.} 
\end{figure} 

\para{Latency}
Two points: (1) going through OS stack is increasing network latency; (2) The bottleneck is on system calls.

Figure 3: The stacked bar chart showing the total latency of TCP/IP, RDMA, shared memory and their components.

\begin{figure}[!ht]
     \centering 
     \includegraphics[width=3.35in]{figures/motivation/eval_baremetal_latency.pdf} 
     \caption{\label{fig:eval_baremetal_latency} The latency of a pair of containers on the same bare metal communicating via IP stack, RDMA and shared memory. Shared memory achieves the lowest latency.} 
\end{figure} 

\para{CPU Usage}

\begin{figure}[!ht]
     \centering 
     \includegraphics[width=3.35in]{figures/motivation/eval_baremetal_cpu.pdf} 
     \caption{\label{fig:eval_baremetal_cpu} The cpu usage of a pair of containers on the same bare metal communicating via IP stack, RDMA and shared memory. Communication via IP stack almost saturates 2 cpu cores.} 
\end{figure} 

% comment 
\fi
