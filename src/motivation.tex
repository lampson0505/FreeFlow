\section{Background and Motivation}
\label{sec:motivation}

In this section, we ...

\subsection{Architecture of containerized applications}

\begin{figure*}[h]  
	\centering   
	\includegraphics[width=6.7in]{figures/deployment-cases}   
	\caption{\label{fig:deploy-cases} Representative running environments of containers.}   
\end{figure*}

%\begin{table} [t!]
%\centering
%\small
%\begin{tabular}{ c || c | c | c | c }
%  \hline
%  Constraint & Case (a) & Case (b) & Case (c) & Case (d) \\ \hline \hline
%  N/A & SharedMem & RDMA & SharedMem & RDMA \\ \hline
%  w/o trust & TCP/IP & TCP/IP & TCP/IP & TCP/IP \\ \hline
%  w/o RDMA NIC & SharedMem & TCP/IP & SharedMem & TCP/IP \\ \hline
%\end{tabular}
%\caption{\label{tab:best-network} The suggested network solution under %different running environments in Figure~\ref{fig:deploy-cases} and constraints.}
%\normalsize
%\end{table}

Nowadays, a containerized application is usually composed by multiple containers. For example, each master and slave node in Hadoop is an individual container; A web service can include layers, such as load balancer, web server,
in-memory cache and backend database, and each layer can be a distributed 
system with multiple containerized nodes. 
These containers are usually 
deployed into a multi-host server cluster, and the deployment is usually 
controlled by a cluster orchestrator, e.g. Mesos~\cite{?}. 
Such architecture makes it easier
to upgrade the nodes or mitigate failures, since a stopped container can be quickly replaced by a new one on the same or another host.

Working as a single application, containers need to exchange data, and the network performance has a huge impact on the overall application performance.
Depending on whether 
containers run on bare-metal hosts or VM hosts, Figure~\ref{fig:deploy-cases}
illustrates four representative cases for a container networking solutions
to handle. 
%As shown later in this section, one networking solution has 
%different efficiency and feasibility in different cases. 
In this paper, 
we focus on the cases (a) and (b) in Figure~\ref{fig:deploy-cases} and discuss how to support the other two cases in the future.

\subsection{Existing container overlay networks}

\begin{figure}[h]  
	\centering   
	\includegraphics[width=2.8in]{figures/overlay-sketch}   
	\caption{\label{fig:overlay} The design of existing container overlay networks (Weave).}   
\end{figure}   

As mentioned in \S\ref{sec:introduction}, {\em Overlay} networking mode 
is preferred to keep the portability and independence of containers.
Despite there are multiple available container overlay networking solutions
in the state of art, e.g. Docker Overlay Network, Weave, Calico, etc., 
their basic idea for offering the connectivity to the containers is similar.
Figure~\ref{fig:overlay} shows the architecture of an overlay container network.
Each hosts runs a software router that connects to one NIC of the host. 
Leveraging the connectivity provided by the inter-host network, the routers
on different hosts can exchange information and speak standard routing protocols.
Inside each host, a virtual interface (veth0) is created in each container and is connected to a virtual interface of the software router in the same host via a software network bridge. The virtual interface of a container is assigned
an overlay IP (e.g. 1.1.1.1 to Container-1) and the host of this overlay IP
is propagated among the software routers in the cluster. Therefore, containers
can send and receive IP packets merely with their overlay IPs, agnostic to
the real location of the other side. For instance, when a IP packet from
Container-1 to Container-3 is send to Router-1, the latter knows 1.1.1.2
(Container-3's overlay IP) is on Host-2, so that Router-1 will encapsulate 
an IP header with destination IP 192.168.1.2 (Host-2's IP) on top of the packet
and send it to Host-2. Host-2 decapsulates the packet after receiving it
and deliver it to Container-3.  

There are two components which significantly limit the performance
of current container overlay networks. The first one is the network bridge
connecting containers and software routers. It is a software implementation
of a subset of the ANSI/IEEE 802.1d standard~\cite{?} which forwards packets
according to Ethernet addresses. Tremendous CPU resource is 



The Linux bridge code implements a subset of the ANSI/IEEE 802.1d standard.


A bridge is a way to connect two Ethernet segments together in a protocol independent way. Packets are forwarded based on Ethernet address, rather than IP address (like a router). Since forwarding is done at Layer 2, all protocols can go transparently through a bridge.

 [1]. The original Linux bridging was first done in Linux 2.2, then rewritten by Lennert Buytenhek. The code for bridging has been integrated into 2.4 and 2.6 kernel series.



\subsection{Existing Solutions of Container Networking}

P1: [Details and related work for existing container network solutions]There are two networking modes to inter-connect containers over multiple hosts:

\begin{itemize}
  \item Host mode: breaking the portability and independency
  \item Overlay mode: keeping the portability and independency
  \begin{itemize}
  \item Docker native overlay
  \item Weave
  \item Caliber (Optional)
  \end{itemize}  
\end{itemize}


Conclusions:
\begin{itemize}
  \item The intra-host throughput of existing solutions are less than 40Gb/s.
  \item The throughput, latency of two containers with host mode is close to the throughput of processes.
  \item All the existing solutions put a heavy load on CPU. CPU is the main bottleneck for throughput.
\end{itemize}

\subsection{Opportunities to build a better container networks}
P1: [Comparing container networking with other solutions]: In Sec 1, we have already seen the poor performance and high overhead of current container networking solution. Given containers are essentially processes, there are multiple better ways for two containers to communicate with each other.

\begin{itemize}
  \item Shared memory
  \item RDMA
  \item DPDK (not sure whether we have time to do it)
\end{itemize}

\subsubsection{Intra-host communication}

Throughput:
     \begin{figure}[ht]
     \centering 
     \includegraphics[width=0.32\textwidth]{figures/motivation/eval_baremetal_thr.pdf}      
     \label{fig:eval_baremetal_thr}
     \caption{} 
     \end{figure}

Latency:    
     \begin{figure}[ht]
     \centering 
     \includegraphics[width=0.32\textwidth]{figures/motivation/eval_baremetal_latency.pdf}      
     \label{fig:eval_baremetal_latency}
     \caption{} 
     \end{figure}

CPU usage:     
     \begin{figure}[ht]
     \centering 
     \includegraphics[width=0.32\textwidth]{figures/motivation/eval_baremetal_cpu.pdf}      
     \label{fig:eval_baremetal_cpu}
     \caption{} 
     \end{figure}
     
\begin{itemize}
  \item Communication via bridge mode only achieves 27Gb/s throughput, 1 ms latency but uses near to 200\% of cpu. (Host mode and overlay mode are similar.)
  \item RDMA only improves the throughput to 40Gb/s for containers on the same machine, the latency is still 1ms, though it has a low cpu usage.
  \item Shared memory can achieve near-to-memory-bandwidth throughput, lowest latency, but still burns some cpu. 
\end{itemize}


\subsubsection{Inter-host communication}



\subsection{Our Goals}
P1: [Goal] We want to build a container overlay networking solution which can guarantee the best network performance while keeping portability and independency for containers.

P2: [Specific requirements, challenges]
\begin{itemize}
  \item Smartly choosing network solutions (e.g. shared- memory, RDMA, DPDK, etc.) according to multiple factors.
  \item Making the network solution selection and switching be transparent to containers.
  \item IP assignments is independent to container's locations.
\end{itemize}

%%% commented content 
\iffalse

\begin{figure}[ht]
     \centering 
     \includegraphics[width=0.4\textwidth]{figures/motivation/eval_exist_bw.pdf} 
     \label{fig:eval_exist_bw}
     \caption{The intra-host throughput of existing solutions. Docker-host is in host mode; Docker0 is in bridge mode; Weave is in overlay mode.} 
\end{figure} 

\begin{figure}[ht]
     \centering 
     \includegraphics[width=0.4\textwidth]{figures/motivation/eval_exist_latency.pdf} 
     \label{fig:eval_exist_latency}
     \caption{The intra-host latency of existing solutions. Docker-host is in host mode; Docker0 is in bridge mode; Weave is in overlay mode.} 
\end{figure} 

\begin{figure}[ht]
     \centering 
     \includegraphics[width=0.4\textwidth]{figures/motivation/eval_exist_cpu.pdf} 
     \label{fig:eval_exist_cpu}
     \caption{The cpu usage of existing solutions. Docker-host is in host mode; Docker0 is in bridge mode; Weave is in overlay mode.} 
\end{figure} 

\begin{figure}[ht]
     \centering 
     \includegraphics[width=0.4\textwidth]{figures/motivation/eval_bw_cpu.pdf}      
     \label{fig:eval_bw_cpu}
     \caption{} 
\end{figure}


* Figures:

* Throughput comparison (intra- and inter-host)

* CPU comparison

* Latency comparison

In this section, we present measurement results that demonstrates the network performance and resource overhead of different optional network channels. We focus on network throughput and latency when containers are located in the same or different hosts. 




\subsection{Opportunities}

\subsection{Challenges}

\subsection{The ineffiencies in container networking}

We measure the throughput and latency of TCP/IP, Shared-Mem and RDMA under cases
(a) (b) (c) and (d).

\harry{RDMA and Shared-Mem is missing under (c) and (d)}.

\para{The measurement setup}
(1) two bare metals: 40Gbps NICs.
(2) two VMs on top of the two bare metals.
(3) two VMs from Azure or EC2.

\subsection{Intra-host performance}
\subsubsection{throughput vs. latency vs. cpu usage}

\begin{figure*}[t]
     \centering 
     %\begin{subfigure}[t]{0.25\textwidth}
     \begin{subfigure}[t]
     \centering 
     \includegraphics[width=0.32\textwidth]{figures/motivation/eval_baremetal_thr.pdf}      
     %\label{fig:eval_baremetal_thr}
     %\caption{} 
     \end{subfigure}      
     \begin{subfigure}[t]
     \centering 
     \includegraphics[width=0.32\textwidth]{figures/motivation/eval_baremetal_latency.pdf}      
     %\label{fig:eval_baremetal_latency}
     %\caption{} 
     \end{subfigure}      
     \begin{subfigure}[t]
     \centering 
     \includegraphics[width=0.32\textwidth]{figures/motivation/eval_baremetal_cpu.pdf}      
     %\label{fig:eval_baremetal_cpu}
     %\caption{} 
     \end{subfigure}           
     \label{fig:eval_baremetal_thr_latency_cpu}
     \caption{} 
\end{figure*} 

\begin{itemize}
  \item Communication via IP stack only achieves 27Gb/s throughput, 1 ms latency but uses near to 200\% of cpu.
  \item RDMA only improves the throughput to 40Gb/s for containers on the same machine, the latency is still 1ms, though it has a low cpu usage.
  \item Shared memory can achieve near-to-memory-bandwidth throughput, lowest latency, but still burns some cpu. 
\end{itemize}


\subsubsection{Host-mode vs. bridge mode}

\begin{figure}[!ht]
     \centering 
     \includegraphics[width=3.35in]{figures/motivation/eval_bw_host_bridge.pdf} 
     \label{fig:eval_bw_host_bridge}
     \caption{ Host-mode vs. bridge mode vs. RDMA vs. shared memory.} 
\end{figure} 

\begin{itemize}
  \item Host-mode provides a better performance of 38 Gb/s. 
\end{itemize}

\subsection{Intra-host Network Performance}

\para{Throughput}

Three points: (1) TCP throughput is limited; (2) the bottleneck is CPU rather than memory bus for TCP/IP; (3) the bottleneck is NIC CPU for RDMA. (4) For intra-host cases, shared memory has the best performance.

Figure 1: throughput of single src-dst pair. Bar figure: x-axis: TCP/IP, RDMA and shared memory; y-axis: throughput;

Figure 2(a): throughput of multiple src-dst pair. Line figure: x-axis: number of pairs,; y-axis: throughput; Four lines: TCP/IP, RDMA, shared memory and memory bus.

Figure 2(b): CPU utilization. Line figure: x-axis: number of pairs,; y-axis: CPU utilization; Three lines: TCP/IP, RDMA, shared memory.

Figure 2(c): NIC CPU utilization. Line figure: x-axis: number of pairs,; y-axis: NIC CPU utilization; Three lines: TCP/IP, RDMA, shared memory.

\begin{figure}[!ht]
     \centering 
     \includegraphics[width=3.35in]{figures/motivation/eval_baremetal_thr.pdf} 
     \caption{\label{fig:eval_baremetal_thr} The throughput of a pair of containers on the same bare metal communicating via IP stack, RDMA and shared memory. Communication via shared memory is close to the memory bandwidth.} 
\end{figure} 

\para{Latency}
Two points: (1) going through OS stack is increasing network latency; (2) The bottleneck is on system calls.

Figure 3: The stacked bar chart showing the total latency of TCP/IP, RDMA, shared memory and their components.

\begin{figure}[!ht]
     \centering 
     \includegraphics[width=3.35in]{figures/motivation/eval_baremetal_latency.pdf} 
     \caption{\label{fig:eval_baremetal_latency} The latency of a pair of containers on the same bare metal communicating via IP stack, RDMA and shared memory. Shared memory achieves the lowest latency.} 
\end{figure} 

\para{CPU Usage}

\begin{figure}[!ht]
     \centering 
     \includegraphics[width=3.35in]{figures/motivation/eval_baremetal_cpu.pdf} 
     \caption{\label{fig:eval_baremetal_cpu} The cpu usage of a pair of containers on the same bare metal communicating via IP stack, RDMA and shared memory. Communication via IP stack almost saturates 2 cpu cores.} 
\end{figure} 

% comment 
\fi