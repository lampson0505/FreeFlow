\section{Design} \label{sec:design}

\begin{figure}[t!] 
     \centering 
     \includegraphics[width=3.2in]{figures/system-arch.png} 
    \caption{\label{fig:sysarch} The overall system architecture of~\sysname. Gray boxes are building blocks of~\sysname.} 
\end{figure} 

This section presents the high-level design of \sysname. We introduce
the requirements and concerns in the designs of control-plane, data-plane
and programming interface, and explain how \sysname makes the design choices
and the reason behind them.

\subsection{Overview}

Our goal is to design a completed network solution which can enable containers
to communicate with portability, isolation and high performance at the same time.
This solution has three main components: control-plane, data-plane and programing
abstraction. 

A flexible control-plane is a guarantee of portability and isolation. 
It should permit a container to register its own IP addresses in the network
from any host, and also should automatically configure and update the routing 
to each container. Different from existing overlay network solutions of 
containers, we also require the control-plane to be able to select 
data-plane protocols in real-time according to multiple factors, 
e.g. container locations, hardware capabilities, etc..

Data-plane should always promise the best network performance between 
containers. From measurements (Figure~\ref{XXX}), 
we see that shared-memory has the highest
bandwidth, lowest latency, smallest NIC load and modest CPU load. 
Therefore, shared-memory is favorable to be used as the data-plane 
mechanism between two containers on the same host. 
The data-plane for inter-host communication
depends on the hardware capability. RDMA is preferred to TCP/IP if the NICs
on the hosts have the support.

A network programming abstraction defines how communication is performed.
People have designed multiple network abstractions used in different scenarios.
For example, the most popular network abstraction for TCP/IP is Socket~\cite{?},
Verbs~\cite{?} is widely accepted for RDMA, and MPI~\cite{?} is a standard for
message-passing in distributed memory and parallel computers. We require that
the network programming abstraction must at least have two
properties. First, it facilitates containers to access the network and 
transparently utilize the data-plane mechanisms, and second, it is general 
enough to support existing programming interfaces on top of it for backward 
compatibility.

The design of~\sysname fully considers the requirements from all these three
components.

\subsection{The architecture of FreeFlow}

Figure~\ref{fig:sysarch} shows the architecture and design choices on each 
components of~\sysname. Network orchestrator is the centralized control-plane
of~\sysname. The local network agent running in each host coordinates
multiple data-plane options according to the information and configurations feed
from the network orchestrator. We choose standard RDMA programming interface --
Verbs API -- as the programming API in the whole solution. Next, we will explains
our design decisions in details. 

\para{Centralized control-plane:} \sysname inherently needs a (conceptually)
centralized control-plane because the IP assignment, routing configuration and 
data-plane mechanism decisions are all computed from the global status of the
container cluster. The network orchestrator of~\sysname maintains three kinds
of global information: the location of each container, the assigned IP of each
container and the capabilities of host NICs.

\para{Integrated Data-plane:} \sysname's local network agent on each host 
obtains the container location information from the network orchestrator. 
It decides to use shared-memory to communicate if two containers are on the
same host, otherwise, it will conduct the communicate going through 
the NIC of its host. Depending on the capability of the NIC, the traffic 
between two hosts can be delivered via RDMA, DPDK or TCP/IP.

\para{RDMA-Verbs based programming abstraction:} We decide to adopt 
RDMA-Verbs API for three reasons. First, in data centers, most of
the applications that have strong desires to high networking performance
are written or are migrating to RDMA based networking abstraction, so that
\sysname can naturally support these applications; Next, Verbs API is a 
widely accepted and used industrial standard; Finally, Verbs API is proven
to be a general and low-level programing interface, and there are already
adaptation layer libraries available to support TCP/IP~\cite{?}
and MPI~\cite{?} on top of Verbs. Given that Verbs API directly talks to
NIC, \sysname provides a virtual RDMA NIC on each container. One virtual NIC
can talk with the process of its container with RDMA protocols and hide
the decisions of local network agent and the actual data-plane mechanism
that is used. 